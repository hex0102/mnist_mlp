import torch
import torchvision
from torchvision import datasets, transforms
import torch.nn as nn
import torch.nn.init as init
from torch.autograd import Variable
import torch.utils.data as Data
from mlp_class import MLP
import matplotlib.pyplot as plt
from utils import *

import numpy as np

#GPU configurations
torch.set_num_threads(2)
torch.cuda.set_device(0)


torch.manual_seed(2)

N_in = 784
N_out = 10
EPOCH = 1
BATCH_SIZE = 128
LR = 0.1#0.1
DOWNLOAD_MNIST = True
en_plot = 0

load_model_train = 1
save_model = 0
load_model = 0
collect_grads = 1
lamb  = 20
#flat minimal parameters
SMALL_D = 0.000001
EPSILON = torch.cuda.FloatTensor([0.16])
K = 100
wrange = 0.4
N_layer = 2
N_hidden = 20




SI = 1  # reminder for sign bit
IL = 3  # IL = torch.IntTensor([4])   Not including the signed bit
FL = 3  # FL = torch.IntTensor([12])
nonideal_train = 0  # fixed point train
nonideal_inference = 0  # fixed point inference

# enabling ntv which increases bit flip probability
en_ntv_inference = 0
en_ntv_train = 0
p_flip = 0.001
flip_len = IL + FL - 1 # flip all bits

input_grads = []  # gradients are inside
weight_grads = []
y_out = []
y_in = []

'''
train_data = torchvision.datasets.MNIST(
    root='./mnist/',
    train=True,
    transform=torchvision.transforms.ToTensor(),
    download=DOWNLOAD_MNIST,
)

test_data = torchvision.datasets.MNIST(root='./mnist/', train=False)

train_loader = Data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)

test_x = Variable(torch.unsqueeze(test_data.test_data, dim=1), volatile=True).type(torch.FloatTensor)[:2000] / 255.
test_y = test_data.test_labels[:2000]
'''
train_data = torchvision.datasets.MNIST(
    root = './mnist/',
    train=True,
    transform=transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]),
    download=DOWNLOAD_MNIST,
)


#test_data = torchvision.datasets.MNIST(root='./mnist/', train=False,transform=transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]))

train_loader = Data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)

test_loader = torch.utils.data.DataLoader(
    datasets.MNIST('./mnist/', train=False, transform=transforms.Compose([
                       transforms.ToTensor(),
                       transforms.Normalize((0.1307,), (0.3081,))
                   ])),
    batch_size=BATCH_SIZE, shuffle=False)




# defining hooks
def fixed_point_hook(self, input, output):
    '''
    if step % 50 == 0 and 'ReLU' == self.__class__.__name__:
        print('sum > 1 :', ((output.data)>1).sum())
        print('sum > 2 :', ((output.data) > 2).sum())
        print('sum > 3 :', ((output.data) > 3).sum())
        print('sum > 3.9 :', ((output.data) > 3.9).sum())
    '''

    # print('Inside ' + self.__class__.__name__ + ' forward')
    # print('Inside ' + self.__class__.__name__ + ' backward')
    apply_format_inplace('FXP', output.data, IL, FL)


def fixed_point_back_hook(self, grad_in, grad_out):
    # print('Inside ' + self.__class__.__name__ + ' backward')
    '''
    print('grad_output size:', grad_out[0].size())
    print('grad_input size:', grad_in[0].size())
    print('grad_input size:', grad_in[1].size())
    print('grad_input size:', grad_in[2].size())

    print('grad_output size:', grad_out[0].size())
    print(grad_out[0])
    print('grad_input size:', grad_in[0].size())
    print(grad_in[0])
    print('grad_input size:', grad_in[1].size())
    print('grad_input size:', grad_in[2].size())
    print('grad_input tuple size: ', grad_in.__len__())
    '''
    # grads[self.__class__.__name__] = grad_in[0].data;
    #grads.append(grad_in[0].data);
    if grad_in.__len__() > 1:
        # print('grad_input size:', grad_in[1].size())
        apply_format_inplace('FXP', grad_in[0].data, IL, FL)
        apply_format_inplace('FXP', grad_in[1].data, IL, FL)  # weight
        apply_format_inplace('FXP', grad_in[2].data, IL, FL)  # bias

        # print(type(grad_in[0]))
        # print(type(grad_in[1]))
        # print(type(grad_in[2]))
        return grad_in


def get_ingrads(self, grad_in, grad_out):
    input_grads.append(grad_in[0].data)
    if grad_in.__len__() > 1:
        weight_grads.append(grad_in[1].data)
    #output_grads.append(grad_out)

def get_y(self, input, output):
    y_out.append(input)
    y_in.append(output)


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        xavier(m.weight.data)

def xavier(initial_data):
    initial_data.zero_()
    temp = wrange*(torch.rand(initial_data.size()).cuda() - 0.5)
    initial_data.add_(temp)


def mse_loss(x, y, size_average=False, per_element=True):
    #print(type(y))
    #print(y.data.size())
    #print(y.data.type())
    '''
    y.data = y.data.unsqueeze(1)
    #print(y.data.size())
    y_onehot = torch.cuda.FloatTensor(BATCH_SIZE, 10)
    y_onehot.zero_()
    y_onehot.scatter_(1, y.data, 1)
    y.data = y_onehot
    #print(x.size())
    #print(y.size())
    loss_per_element = (x - y) ** 2
    if per_element:
        return loss_per_element.mean(1)
    if size_average:
        return loss_per_element.mean()
    return loss_per_element.sum()
    '''

    y.data = y.data.unsqueeze(1)
    y_onehot = torch.cuda.FloatTensor(BATCH_SIZE, 10)
    y_onehot.zero_()
    y_onehot.scatter_(1, y.data, 1)
    label = y_onehot
    label = Variable(label)
    loss_per_element = (x- label) ** 2
    if per_element:
        return loss_per_element.mean(1,keepdim=True)
    if size_average:
        return loss_per_element.mean()
    return loss_per_element.sum()



if load_model_train:
    print('Loading model......')
    mlp = torch.load('mlp_mnist_fms.pkl')
else:
    mlp = MLP().cuda()
    mlp.apply(weights_init)

optimizer = torch.optim.SGD(mlp.parameters(), lr=LR)  # , momentum=0.9)
loss_func = nn.CrossEntropyLoss().cuda()

params = mlp.state_dict()

# add backward hooker on each layer, to regulate the gradient to fixed point representation
if (collect_grads):
    mlp.hidden1.register_backward_hook(get_ingrads)
    mlp.activation.register_backward_hook(get_ingrads)
    mlp.hidden2.register_backward_hook(get_ingrads)
    mlp.activation2.register_backward_hook(get_ingrads)
    # add foward hooker on each layer, , to regulate the layer out to fixed point representation
    mlp.hidden1.register_forward_hook(get_y)
    mlp.activation.register_forward_hook(get_y)
    mlp.hidden2.register_forward_hook(get_y)
    mlp.activation2.register_forward_hook(get_y)

# >>>>>>>>Dump neural network weight to list and do initialization
if not load_model_train:
    linear = list(mlp.hidden1.parameters())
    init.constant(linear[1].data, 0)
    linear2 = list(mlp.hidden2.parameters())
    init.constant(linear2[1].data, 0)
    out = list(mlp.out.parameters())
    init.constant(out[1].data, 0)

    init.normal(linear[0].data, mean=0, std=0.01)
    init.normal(linear2[0].data, mean=0, std=0.01)
    init.normal(out[0].data, mean=0, std=0.01)

train_loss_y = np.array([])
train_loss_x = np.array([])
i = 0

dyds=[]
dydy=[]

for epoch in range(EPOCH):
    avg_loss = np.array([])
    total_b = torch.cuda.FloatTensor(1).zero_()
    for step, (x, y) in enumerate(train_loader):
        b_x = Variable(x.cuda(), requires_grad=True)
        b_y = Variable(y.cuda())
        BATCH_SIZE = b_x.size()[0]
        y_out = []
        y_in = []
        output = mlp(b_x)


        #first backward to get dydw and ks
        dydw = []
        #print(len(y_out)
        ks = []
        #ky= []
        for i in range(10):
            ind = torch.zeros(b_x.size()[0], 10).cuda()
            ind[:, i] = 1
            input_grads = [] #reset input grads list
            #output_grads = []
            output.backward(ind, retain_variables=True)
            ks.append(input_grads)
            #ky.append(output_grads)
            dyidw=[]
            for j in range(N_layer):
                #print(j)
                y_part = y_out[2*j][0].data
                y_part = y_part.unsqueeze(1)
                ys = input_grads[N_layer-2*j]    #need to considering
                ys = ys.unsqueeze(N_layer)
                #print(ys.size())
                #print(y_part.size())
                yw = torch.bmm(ys,y_part)

                #yw.add_(torch.mul((torch.abs(yw)<SMALL_D),SMALL_D))
                #dyidw.append(yw+torch.mul((torch.abs(yw)<SMALL_D).type(torch.cuda.FloatTensor),SMALL_D))
                if j != N_layer - 1:
                    small_index = torch.abs(yw) < SMALL_D
                    yw[small_index] = torch.sign(yw[small_index]+0.0000000000001)* SMALL_D
                else:
                    #print(yw[:,i,:].size
                    temp = yw[:, i, :]
                    small_index = torch.abs(temp) < SMALL_D
                    temp[small_index] = torch.sign(temp[small_index]+0.0000000000001) * SMALL_D
                    #yw[:, i, :].add_(torch.sign(yw[:, i, :])*((torch.abs(yw[:, i, :]) < SMALL_D).type(torch.cuda.FloatTensor) * SMALL_D))
                    yw[:, i, :] = temp
                dyidw.append(yw)

            dydw.append(dyidw)
            mlp.zero_grad()
            #for j in range(2):
            #    y_out[2*j]
        t1 = []
        for n in range(N_layer):
            t1.append(dydw[0][n].clone().zero_())
            for m in range(10):
                #t1[n].add_(torch.mul(dydw[m][n],dydw[m][n]))
                t1[n].add_((dydw[m][n] * dydw[m][n]))
        #t1 [0][50]  #dydw[0][0][50][18]
        t2 = torch.zeros(10,BATCH_SIZE).cuda()
        #checked

        for m in range(10):
            for n in range(N_layer):
                #print(t2[m].size())
                #print(torch.sum(torch.sum(torch.abs(torch.div(dydw[m][n],torch.sqrt(t1[n]))),2),1).size())
                #t2[m] += torch.sum(torch.sum(torch.abs(torch.div(dydw[m][n],torch.sqrt(t1[n]))),2),1)
                t2[m].add_(torch.sum(torch.sum(torch.abs(torch.div(dydw[m][n],torch.sqrt(t1[n]))),2),1))

        t2.transpose_(0,1)

        t3 = torch.sum(torch.mul(t2,t2),1)
        #print(t3.size()) #[128]
        #print(EPSILON)
        delta_w = []
        delta_min = torch.cuda.FloatTensor(N_layer,BATCH_SIZE);
        #eps_t3 = torch.sqrt(torch.mul(torch.reciprocal(t3),EPSILON)).unsqueeze(1).unsqueeze(1)
        eps_t3 = torch.sqrt(EPSILON/t3).unsqueeze(1).unsqueeze(1)
        #print(eps_t3.size()) #[128,1,1]
        avg_b = torch.cuda.FloatTensor(1).zero_()
        for n in range(N_layer):
            #delta_w.append(torch.mul(torch.sqrt(torch.reciprocal(t1[n])),eps_t3))
            delta_w.append(torch.div(eps_t3,torch.sqrt(t1[n])))
            #print(delta_w[n].size())
            #print(delta_w[n])
            avg_b.add_(torch.mean(torch.sum(torch.sum(-1*torch.log(delta_w[n])/torch.cuda.FloatTensor([3.3219]),1),1),0))
            delta_min[n]=torch.min(torch.min(delta_w[n],2)[0],1)[0]
            #delta_min.append(torch.min(torch.min(delta_w[n],1)[0],1)[0])
        #print(avg_b)
        total_b.add_(avg_b)
        d_min = (torch.min(delta_min,0)[0]).unsqueeze(1).unsqueeze(1)*K
        #print(d_min)
        #print(d_min.size())
        #d_min.transpose_(0,1)
        #for n in range(N_layer):



        insignificant = []
        weights = torch.cuda.FloatTensor(N_layer,BATCH_SIZE)
        for n in range(N_layer):
            insignificant.append(torch.lt(delta_w[n], d_min).type(torch.cuda.FloatTensor))
            weights[n] = torch.sum(torch.sum(insignificant[n],1),1)
        weights = torch.sum(weights,0)
        #print(weights)
            #t1[n]=torch.mul(t1[n],insignificant[n].type(torch.cuda.FloatTensor))
        #print(weights) #128   checked
        #dydw[m][n]-- b_s*higher*lower
        #update variables after having marked
        t2 = torch.zeros(10,BATCH_SIZE).cuda()
        for m in range(10):
            for n in range(N_layer):
                t2[m] += torch.sum(torch.sum(torch.abs(torch.div(dydw[m][n]*insignificant[n],torch.sqrt(t1[n]))),1),1)


        t2.transpose_(0,1)
        t3 = torch.sum(torch.mul(t2,t2),1)  # BATCH_SIZE
        #print(t3)
        t2.transpose_(0, 1) #

        # print(t2.size()) torch.Size([10, 128])
        # print(insignificant[0].size()) torch.Size([128, 1000, 784])

        #for calculation, we change dydw structure
        '''
        dydw_shape = []
        for n in range(len(dydw[0])):
            dydw_tensor = dydw[0][n]
            for m in range(1, 10):
                dydw_tensor = torch.cat((dydw_tensor,dydw[m][n]),1)
            dydw_shape.append(dydw_tensor)
        '''

        #print(insignificant[n].size())  torch.Size([128, upper, lower])
        #print(t1[0].size())
        # t2 [BATCH_SIZE*N_output]   [Batch_size*upper*lower]
        #dydw[m][n]           batch_size*upper*lower
        #print(t2[m].size())  torch.Size([128])

        b  = []
        t4 = []
        for k in range(N_out):
            temp = []
            for n in range(N_layer):
                t4 = dydw[0][n].clone().zero_().cuda()
                #temp.append(torch.div(dydw[m][n], t1[n]))
                for m in range(N_out):
                    t4.add_(t2[m].unsqueeze(1).unsqueeze(1)*insignificant[n]*torch.sign(dydw[m][n])*\
                    ((k==m)*t1[n] - dydw[k][n]*dydw[m][n])/torch.pow(t1[n],1.5))
                temp.append(torch.div(dydw[m][n], t1[n])+ weights.unsqueeze(1).unsqueeze(1)*t4/(t3.unsqueeze(1).unsqueeze(1)))
            b.append(temp)

        #print(b)
        #print(len(b))
        #print(len(b[0]))
        #print(len(b[0][0]))
        #print(len(b[0][0][0]))
        #print((b[0][1][0][0]))
        del t1
        del t2
        del t3

        #params = mlp.state_dict()
        #Forward pass

        params = mlp.state_dict()
        param_list = list(params.values())

        weights = []
        bias = []
        for i in range(N_layer):
            weights.append(param_list[2*i])
            bias.append(param_list[2*i+1])



        ry = []
        ry.append(torch.cuda.FloatTensor(N_out,BATCH_SIZE,N_in).zero_())
        ry.append(torch.cuda.FloatTensor(N_out,BATCH_SIZE,N_hidden).zero_())
        ry.append(torch.cuda.FloatTensor(N_out,BATCH_SIZE,1).zero_())
        rs = []

        #second forward pass
        for k in range(N_out):
            temp = []
            for u in range(1,N_layer):
                #print(ry[u - 1][k].size())
                #print(torch.unsqueeze(torch.transpose(weights[u - 1], 0, 1),0).size())
                #print(torch.transpose(insignificant[u - 1],1,2).size())


                #print(b[k][u-1].size()) #torch.Size([128, 500, 784])
                #print((y_out[2*u-2][0].data.unsqueeze(2)).size())
                #   bmm(128*1*784, 128*784*500 * 1*784*500 ).squeeze  +
                rs_tmp = torch.squeeze(torch.bmm(torch.unsqueeze(ry[u - 1][k],1), torch.transpose(insignificant[u - 1],1,2)*torch.unsqueeze(torch.transpose(weights[u - 1], 0, 1),0)),1)+ \
                torch.squeeze(torch.bmm(b[k][u-1]*insignificant[u - 1],y_out[2*u-2][0].data.unsqueeze(2)))
                #print(rs_tmp.size())
                #rs_tmp = torch.mm(ry[u - 1][k], torch.transpose(weights[u-1], 0, 1))+ \
                #torch.squeeze(torch.bmm(b[k][u-1],y_out[2*u-2][0].data.unsqueeze(2)))
                temp.append(rs_tmp)

                #print(y_out[2*u][0].data.size())
                ry[u][k] = rs_tmp*((1-y_out[2*u][0].data)*y_out[2*u][0].data)  #need review
                #BATCH_SIZE*HIDDEN*
            #print(ry[u][k].size())
            #print(weights[u][k].unsqueeze(1).size())
            #print(torch.index_select(b[k][u],1,torch.cuda.LongTensor([k])).size())

            #print(torch.mm(ry[u][k],weights[u][k].unsqueeze(1)).size())
            #print(torch.squeeze(torch.bmm(torch.index_select(b[k][u],1,torch.cuda.LongTensor([k])), y_out[2*u][0].data.unsqueeze(2))).size())
            #rs_tmp = torch.mm(ry[u][k],weights[u][k].unsqueeze(1))\
                     #+torch.squeeze(torch.bmm(torch.index_select(b[k][u],1,torch.cuda.LongTensor([k])), y_out[2*u][0].data.unsqueeze(2)),2) #should be caraful
            #print(rs_tmp.size())
            #print(insignificant[u].size())#torch.Size([128, 10, 500])
            #print(torch.bmm(ry[u][k].unsqueeze(1), torch.transpose(((weights[u][k].unsqueeze(1)).unsqueeze(0))*torch.index_select(insignificant[u],1,torch.cuda.LongTensor([k])),1,2)).size())
            #print(torch.index_select(b[k][u],1,torch.cuda.LongTensor([k])).size())
            rs_tmp = torch.squeeze(torch.bmm(ry[u][k].unsqueeze(1), ((weights[u][k].unsqueeze(1)).unsqueeze(0))*torch.transpose(torch.index_select(insignificant[u],1,torch.cuda.LongTensor([k])),1,2)),2)\
                     + torch.squeeze(torch.bmm(torch.index_select(b[k][u],1,torch.cuda.LongTensor([k]))*torch.index_select(insignificant[u],1,torch.cuda.LongTensor([k])), y_out[2*u][0].data.unsqueeze(2)),2) #should be caraful


            #rint(rs_tmp.size())
            temp.append(rs_tmp)
            rs.append(temp)
            ry[u+1][k] = rs_tmp * (1- torch.index_select(y_in[2*u+1].data,1 ,torch.cuda.LongTensor([k])))*torch.index_select(y_in[2*u+1].data,1 ,torch.cuda.LongTensor([k]))


        #print(len(rs))


        rdks = []
        rdky_top = []
        rdw = []


        for k in range(N_out):
            rdks_k = []
            rdw_k = []
            temp = torch.index_select(y_in[3].data,1,torch.cuda.LongTensor([k]))

            #print((rs[k][-1]).size())
            #print(((1-2*temp)*temp*(1-temp)).size())
            rdks_tmp=(rs[k][-1]*(1-2*temp)*temp*(1-temp))  #[128*1] rs[2*k+1]
            #print(rdks_tmp.size()) #torch.Size([128, 1])
            #print(weights[-1][k].size()) #500  checked
            #print(torch.index_select(b[k][-1],1,torch.cuda.LongTensor([k])).size()) #128*1*500
            #print(ks[k][0].size()) #128*10
            #print(torch.unsqueeze(torch.index_select(ks[k][0],1,torch.cuda.LongTensor([k])),1).size())
            #rdks 128*1*500 * 500        +  128*1*500 * 128*1
            #temp=rdks_tmp*weights[-1][k]*torch.index_select(insignificant[-1],1,torch.cuda.LongTensor([k]))+ \
                 #torch.squeeze(torch.index_select(b[k][-1],1,torch.cuda.LongTensor([k])),1)*torch.index_select(ks[k][0],1,torch.cuda.LongTensor([k]))
            temp=torch.squeeze(torch.bmm(rdks_tmp.unsqueeze(2), ((weights[-1][k].unsqueeze(0)).unsqueeze(0))*torch.index_select(insignificant[-1],1,torch.cuda.LongTensor([k]))),1)+ \
                 torch.squeeze(torch.index_select(b[k][-1],1,torch.cuda.LongTensor([k]))*torch.index_select(insignificant[-1],1,torch.cuda.LongTensor([k])),1)*torch.index_select(ks[k][0],1,torch.cuda.LongTensor([k]))
            #print((rdks_tmp*weights[-1][k]).size())
            #print(torch.index_select(b[k][-1],1,torch.cuda.LongTensor([k])).size())
            #print(torch.unsqueeze(torch.index_select(ks[k][0],1,torch.cuda.LongTensor([k])),1).size())
            rdky_top.append(temp) #rdky has ten 3d tensors with (128*1*500)
            #(batch_size * hidden) * (batch_size*1)  + (batch_size*hidden)*(batch_size*1)
            #print(rdks_tmp.size())
            #print(y_out[-2][0].data.size())
            #print(ry[-2][k].size())
            #print(torch.index_select(ks[k][0],1,torch.cuda.LongTensor([k])).size())
            temp = (y_out[-2][0].data*rdks_tmp + ry[-2][k]*torch.index_select(ks[k][0],1,torch.cuda.LongTensor([k])))*(torch.index_select(insignificant[-1],1,torch.cuda.LongTensor([k])).squeeze(1))
            #rint(temp.size())
            rdw_k.append(temp)

            #(batch_size * (1) * hidden)*batch_size*hidden +

            temp_y = y_in[1].data
            #print(((1 - 2 * temp_y) * temp_y * (1 - temp_y)).size())


            temp = rdky_top[k]*(1-y_out[-2][0].data)*y_out[-2][0].data +\
                   rs[k][-2]*(1-2*temp_y)*temp_y*(1-temp_y)*ks[k][1][0]
            #print((rdks[k] * weights[u][k]).size())


            rdks_k.append(temp)
            #rdks[k] is 128*500  W is
            #
            for u in range(N_layer-2,-1,-1):
                #print(rdks_k[N_layer-2-u].size())
                #print(weights[u].size())
                #print((ks[k][N_layer-2-u+1].size())) #torch.Size([128, 500])
                #print(b[k][u].size()) #torch.Size([128, 500, 784])
                #[batch*hidden]
                #rdky_temp = torch.mm(rdks_k[N_layer-2-u], weights[u]) + \
                       #torch.squeeze(torch.bmm(torch.unsqueeze(ks[k][N_layer-2-u+1],1),b[k][u]),1)
                     #orch.squeeze(torch.index_select(b[k][-1], 1, torch.cuda.LongTensor([k])), 1) * torch.index_select(ks[k][0], 1, torch.cuda.LongTensor([k]))
                #rdky_temp = torch.mm(rdks_k[N_layer-2-u], weights[u]) + \
                       #torch.squeeze(torch.bmm(torch.unsqueeze(ks[k][N_layer-2-u+1],1),b[k][u]),1)
                #print(b[k][u].size())
                #print(insignificant[u].size())
                rdky_temp = torch.squeeze(torch.bmm(torch.unsqueeze(rdks_k[N_layer-2-u],1), insignificant[u]*torch.unsqueeze(weights[u],0)), 1) + \
                       torch.squeeze(torch.bmm(torch.unsqueeze(ks[k][N_layer-2-u+1],1),b[k][u]*insignificant[u]),1)
                #print(rdky_temp.size()) torch.Size([128, 784])
                #print(rdks_k[N_layer - 2 - u].size())
                #print(y_out[u][0].data.size())
                #print(torch.bmm(torch.unsqueeze(rdks_k[N_layer-2-u],2),torch.unsqueeze(y_out[u][0].data,1)).size())
                rdw_temp =  (torch.bmm(torch.unsqueeze(rdks_k[N_layer-2-u],2),torch.unsqueeze(y_out[u][0].data,1))\
                            + torch.bmm(torch.unsqueeze(ks[k][N_layer-2-u+1],2), torch.unsqueeze(ry[u][k],1)))*(insignificant[u])
                rdw_k.append(rdw_temp)

                y_temp = y_out[u][0].data
                #print(ks[k][2*N_layer-2*u-1].size())
                if(u!=0):
                    rdks_k.append((1-y_temp)*y_temp*rdky_temp + rs[k][u-1]*(1-2*y_temp)*y_temp*(1-y_temp)*ks[k][2*N_layer-2*u+1][0])
            rdks.append(rdks_k)
            rdw.append(rdw_k)

        rdw_sum = []
        for n in range(N_layer):
            rdw_sum.append(rdw[0][n].clone().zero_())
        rdw_sum[0] = torch.unsqueeze(rdw[0][0],1)

        #print(rdw_sum[0].size())

        for k in range(1, N_out):
            #print(torch.unsqueeze(rdw[k][0],1).size()) torch.Size([128, 1, 500])
            rdw_sum[0] = torch.cat((rdw_sum[0],torch.unsqueeze(rdw[k][0],1)),1)

        for n in range(1, N_layer):
            for k in range(N_out):
                rdw_sum[n].add_(rdw[k][n])


        #normalize B's gradient to length of E's gradient
        #lyw = torch.sum(torch.sum(rdw_sum[n] * rdw_sum[n], 1), 1)
        lrdw = torch.cuda.FloatTensor(1).zero_()
        lyw = torch.cuda.FloatTensor(1).zero_()
        mlp.zero_grad()
        input_grads = []
        weight_grads = []


        ind = torch.zeros(b_x.size()[0],1).cuda()
        ind[:,:] = 1
        loss = mse_loss(output,b_y)
        #print(loss.data.size())
        #print(ind.size())
        optimizer.zero_grad()
        loss.backward(ind,retain_variables=True)
        #print(output.size())
        #print(b_y.size())


        rdw_updates = []
        for n in range(N_layer):
            #print(rdw_sum[n].size())
            #print(insignificant[N_layer-1-n].size())
            rdw_all_sample = torch.sum(rdw_sum[n]*insignificant[N_layer-1-n],0)
            #print(rdw_all_sample.size())
            rdw_updates.append(rdw_all_sample)
            #print(rdw_all_sample.size())
            lrdw += torch.sum(rdw_all_sample*rdw_all_sample)

        for n in range(N_layer):
            #print(weight_grads[n].size())
            lyw += torch.sum(weight_grads[n]*weight_grads[n]*insignificant[N_layer-1-n])

        #print(lyw)
        #print(lrdw)

        scale = lyw/lrdw


        #print(scale)


        optimizer.step()
        mlp.parameters()


        params = mlp.state_dict()
        param_list = list(params.values())

        weights = []
        bias = []

        #print(lamb*scale*rdw_updates[N_layer-1-0])
        for i in range(N_layer):
            #print(rdw_updates[N_layer-1-i].size())
            #print(insignificant[i].size())
            param_list[2 * i].add_(-1*lamb*scale*rdw_updates[N_layer-1-i])

            #print(param_list[2*i].size())
            #param_list[2*i].add_(rdw)



        #print(loss)
        #loss = loss_func(output, b_y)

        '''
        for i in range(BATCH_SIZE):
            output[i:i+1].backward(torch.ones(2,10),retain_variables=True)
            loss = loss_func(output,b_y)
            mlp.zero_grad()
        '''




        if nonideal_train:
            apply_format_inplace('FXP', linear[0].data, IL, FL)  # weight
            apply_format_inplace('FXP', linear[1].data, IL, FL)  # bias
            apply_format_inplace('FXP', linear2[0].data, IL, FL)  # weight
            apply_format_inplace('FXP', linear2[1].data, IL, FL)  # bias
            apply_format_inplace('FXP', out[0].data, IL, FL)  # weight
            apply_format_inplace('FXP', out[1].data, IL, FL)  # bias

        if en_ntv_train:
            apply_bitflip(linear[0].data, p_flip, IL, FL, flip_len)
            apply_bitflip(linear2[0].data, p_flip, IL, FL, flip_len)
            apply_bitflip(out[0].data, p_flip, IL, FL, flip_len)

        if step % 20 == 0:
            print('Avg_b in a mini_batch: %.4f'% avg_b[0])
            print('Epoch: ', epoch, '| train loss : %.4f' % loss.data[0][0])

        avg_loss = np.append(avg_loss, loss.data[0][0])

        '''
        if step % 120 == 0:
            print('printing!!!')
            train_loss_y = np.append(train_loss_y,loss.data[0])
            train_loss_x = np.append(train_loss_x,i)
            plt.plot(train_loss_x, train_loss_y, 'r-', lw=2)
            plt.pause(0.05)
            #plt.show(block=False)
            i = i + 1

        train_loss_x = train_loss_x.astype(int)
        '''

        '''
        if step % 50 == 0:
            test_output = mlp(test_x)
            pred_y = torch.max(test_output,1)[1].data.squeeze()
            accuracy = sum(pred_y == test_y)/float(test_y.size(0))
            print('Epoch: ',epoch,'| train loss : %.4f' %loss.data[0], '| test accuracy: %.2f' % accuracy)
        '''
    torch.save(mlp, 'mlp_mnist_fms2.pkl')
    print('Saving model......')
    print(total_b)
    # per epoch updating......
    if en_plot:
        train_loss_y = np.append(train_loss_y, np.mean(avg_loss))
        train_loss_x = np.append(train_loss_x, epoch)
        plt.semilogy(train_loss_x, train_loss_y, 'r-', lw=2)
        plt.xlabel('Epoch')
        plt.ylabel('Training error')
        plt.title('IL = 4, FL = 14')
        plt.grid(True)
        plt.pause(0.05)

plt.show()

if save_model:
    print('Saving model......')
    torch.save(mlp, 'mlp_mnist_10epoch.pkl')
    # >>>>>>>>Test neural network performance

if load_model:
    print('Loading model......')
    mlp = torch.load('fms_backup.pkl')

# >>>>>>>>>>>>>>>>>>>>>>>>Inference
linear = list(mlp.hidden1.parameters())
linear2 = list(mlp.hidden2.parameters())
out = list(mlp.out.parameters())

# >>>>>>>>Regulate trained neural network weight into fixed point counterpart, then do inference
if nonideal_inference:

    apply_format_inplace('FXP', linear[0].data, IL, FL)  # weight regulation
    apply_format_inplace('FXP', linear[1].data, IL, FL)  # bias regulation
    apply_format_inplace('FXP', linear2[0].data, IL, FL)  # weight regulation
    apply_format_inplace('FXP', linear2[1].data, IL, FL)  # bias regulation
    apply_format_inplace('FXP', out[0].data, IL, FL)  # weight regulation
    apply_format_inplace('FXP', out[1].data, IL, FL)  # bias regulation
    #apply_format_inplace('FXP', test_x.data, IL, FL)  # input regulation
    # apply_bitflip(linear[0].data,0.5,3,10,13)

if en_ntv_inference:
    print("injecting bit flip fault model...")
    linear[0].data=(apply_bitflip(linear[0].data.cpu(), p_flip, IL, FL, flip_len)).cuda()
    linear2[0].data=apply_bitflip(linear2[0].data.cpu(), p_flip, IL, FL, flip_len).cuda()
    out[0].data=apply_bitflip(out[0].data.cpu(), p_flip, IL, FL, flip_len).cuda()
    # apply_bitflip(x,p,IL,FL,flip_length):

# print(type(test_x)) #<class 'torch.autograd.variable.Variable'>
# print(type(test_x.data)) #<class 'torch.FloatTensor'>


# >>>>>>>>add forward hooker on each layer if non-ideal inference
if nonideal_inference and (not nonideal_train):
    # print('applied!!!')
    mlp.hidden1.register_forward_hook(fixed_point_hook)
    mlp.activation.register_forward_hook(fixed_point_hook)
    mlp.hidden2.register_forward_hook(fixed_point_hook)
    mlp.activation2.register_forward_hook(fixed_point_hook)
    mlp.out.register_forward_hook(fixed_point_hook)

# >>>>>>>> Testing!!!
test_loss = 0
correct = 0
for data, target in test_loader:

    data, target = data.cuda(), target.cuda()
    if nonideal_inference:
        apply_format_inplace('FXP', data, IL, FL)  # input regulation
    data, target = Variable(data, volatile=True), Variable(target)
    output = mlp(data)
    #test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss
    pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability
    correct += pred.eq(target.data.view_as(pred)).cpu().sum()

#test_loss /= len(test_loader.dataset)
print('\nTest set: Accuracy: (%.4f)\n', 100. * correct / len(test_loader.dataset))

print('End of testing!')
