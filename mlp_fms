import torch
import torchvision
import torch.nn as nn
import torch.nn.init as init
from torch.autograd import Variable
import torch.utils.data as Data
from mlp_class import MLP
import matplotlib.pyplot as plt
from utils import *

import numpy as np

#GPU configurations
torch.set_num_threads(2)
torch.cuda.set_device(0)


torch.manual_seed(2)

N_in = 784
N_out = 10
EPOCH = 2
BATCH_SIZE = 128
LR = 1#0.1
DOWNLOAD_MNIST = True
save_model = 0
load_model = 0
en_plot = 0
collect_grads = 1


N_layer = 2
N_hidden = 500
SI = 1  # reminder for sign bit
IL = 5  # IL = torch.IntTensor([4])   Not including the signed bit
FL = 14  # FL = torch.IntTensor([12])
nonideal_train = 0  # fixed point train
nonideal_inference = 0  # fixed point inference

# enabling ntv which increases bit flip probability
en_ntv_inference = 0
en_ntv_train = 0
p_flip = 0.0001
flip_len = IL + FL + SI  # flip all bits

input_grads = []  # gradients are inside
output_grads = []
y_out = []
y_in = []


train_data = torchvision.datasets.MNIST(
    root='./mnist/',
    train=True,
    transform=torchvision.transforms.ToTensor(),
    download=DOWNLOAD_MNIST,
)

test_data = torchvision.datasets.MNIST(root='./mnist/', train=False)

train_loader = Data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)

test_x = Variable(torch.unsqueeze(test_data.test_data, dim=1), volatile=True).type(torch.FloatTensor)[:2000] / 255.
test_y = test_data.test_labels[:2000]

#flat minimal parameters
SMALL_D = 0.000001
EPSILON = torch.cuda.FloatTensor([0.01])
K = 100


# defining hooks
def fixed_point_hook(self, input, output):
    '''
    if step % 50 == 0 and 'ReLU' == self.__class__.__name__:
        print('sum > 1 :', ((output.data)>1).sum())
        print('sum > 2 :', ((output.data) > 2).sum())
        print('sum > 3 :', ((output.data) > 3).sum())
        print('sum > 3.9 :', ((output.data) > 3.9).sum())
    '''

    # print('Inside ' + self.__class__.__name__ + ' forward')
    # print('Inside ' + self.__class__.__name__ + ' backward')
    apply_format_inplace('FXP', output.data, IL, FL)


def fixed_point_back_hook(self, grad_in, grad_out):
    # print('Inside ' + self.__class__.__name__ + ' backward')
    '''
    print('grad_output size:', grad_out[0].size())
    print('grad_input size:', grad_in[0].size())
    print('grad_input size:', grad_in[1].size())
    print('grad_input size:', grad_in[2].size())

    print('grad_output size:', grad_out[0].size())
    print(grad_out[0])
    print('grad_input size:', grad_in[0].size())
    print(grad_in[0])
    print('grad_input size:', grad_in[1].size())
    print('grad_input size:', grad_in[2].size())
    print('grad_input tuple size: ', grad_in.__len__())
    '''
    # grads[self.__class__.__name__] = grad_in[0].data;
    #grads.append(grad_in[0].data);
    if grad_in.__len__() > 1:
        # print('grad_input size:', grad_in[1].size())
        apply_format_inplace('FXP', grad_in[0].data, IL, FL)
        apply_format_inplace('FXP', grad_in[1].data, IL, FL)  # weight
        apply_format_inplace('FXP', grad_in[2].data, IL, FL)  # bias

        # print(type(grad_in[0]))
        # print(type(grad_in[1]))
        # print(type(grad_in[2]))
        return grad_in


def get_ingrads(self, grad_in, grad_out):
    input_grads.append(grad_in[0].data)
    #output_grads.append(grad_out)

def get_y(self, input, output):
    y_out.append(input)
    y_in.append(output)





def mse_loss(x, y, size_average=False, per_element=False):
    loss_per_element = (x - y) ** 2
    if per_element:
        return loss_per_element.sum(1)
    if size_average:
        return loss_per_element.mean()
    return loss_per_element.sum()





mlp = MLP().cuda()
optimizer = torch.optim.SGD(mlp.parameters(), lr=LR)  # , momentum=0.9)
loss_func = nn.CrossEntropyLoss().cuda()

params = mlp.state_dict()

# add backward hooker on each layer, to regulate the gradient to fixed point representation
if (collect_grads):
    mlp.hidden1.register_backward_hook(get_ingrads)
    mlp.activation.register_backward_hook(get_ingrads)
    mlp.hidden2.register_backward_hook(get_ingrads)
    mlp.activation2.register_backward_hook(get_ingrads)
    # add foward hooker on each layer, , to regulate the layer out to fixed point representation
    mlp.hidden1.register_forward_hook(get_y)
    mlp.activation.register_forward_hook(get_y)
    mlp.hidden2.register_forward_hook(get_y)
    mlp.activation2.register_forward_hook(get_y)

# >>>>>>>>Dump neural network weight to list
linear = list(mlp.hidden1.parameters())
init.constant(linear[1].data, 0)
linear2 = list(mlp.hidden2.parameters())
init.constant(linear2[1].data, 0)
out = list(mlp.out.parameters())
init.constant(out[1].data, 0)

init.normal(linear[0].data, mean=0, std=0.01)
init.normal(linear2[0].data, mean=0, std=0.01)
init.normal(out[0].data, mean=0, std=0.01)

train_loss_y = np.array([])
train_loss_x = np.array([])
i = 0

dyds=[]
dydy=[]

for epoch in range(EPOCH):
    avg_loss = np.array([])
    for step, (x, y) in enumerate(train_loader):
        b_x = Variable(x.cuda(), requires_grad=True)
        b_y = Variable(y.cuda())

        output = mlp(b_x)
        dydw = []

        ks = []
        #ky= []
        for i in range(10):
            ind = torch.zeros(b_x.size()[0], 10).cuda()
            ind[:, i] = 1
            input_grads = [] #reset input grads list
            #output_grads = []
            output.backward(ind, retain_variables=True)
            ks.append(input_grads)
            #ky.append(output_grads)
            dyidw=[]
            for j in range(N_layer):
                #print(j)
                y_part = y_out[2*j][0].data
                y_part = y_part.unsqueeze(1)
                ys = input_grads[N_layer-2*j]    #need to considering
                ys = ys.unsqueeze(N_layer)
                yw = torch.bmm(ys,y_part)
                #yw.add_(torch.mul((torch.abs(yw)<SMALL_D),SMALL_D))
                dyidw.append(yw+torch.mul((torch.abs(yw)<SMALL_D).type(torch.cuda.FloatTensor),SMALL_D))
                mlp.zero_grad()
            dydw.append(dyidw)
            mlp.zero_grad()
            #for j in range(2):
            #    y_out[2*j]
        t1 = []
        for n in range(N_layer):
            t1.append(dydw[0][n].clone().zero_())
            for m in range(10):
                t1[n].add_(torch.mul(dydw[m][n],dydw[m][n]))

        t2 = torch.zeros(10,BATCH_SIZE).cuda()


        for m in range(10):
            for n in range(N_layer):
                t2[m] += torch.sum(torch.sum(torch.abs(torch.div(dydw[m][n],torch.sqrt(t1[n]))),1),1)

        t2.transpose_(0,1)

        t3 = torch.sum(torch.mul(t2,t2),1)

        delta_w = []
        delta_min = torch.cuda.FloatTensor(N_layer,BATCH_SIZE);
        eps_t3 = torch.sqrt(torch.mul(torch.reciprocal(t3),EPSILON)).unsqueeze(1).unsqueeze(1)
        for n in range(N_layer):
            delta_w.append(torch.mul(torch.sqrt(torch.reciprocal(t1[n])),eps_t3))
            delta_min[n]=torch.min(torch.min(delta_w[n],1)[0],1)[0]
            #delta_min.append(torch.min(torch.min(delta_w[n],1)[0],1)[0])

        d_min = (torch.min(delta_min,0)[0]).unsqueeze(1).unsqueeze(1)*K
        #d_min.transpose_(0,1)
        #for n in range(N_layer):


        insignificant=[]
        weights = torch.cuda.FloatTensor(N_layer,BATCH_SIZE)
        for n in range(N_layer):
            insignificant.append(torch.lt(delta_w[n], d_min).type(torch.cuda.FloatTensor))
            weights[n] = torch.sum(torch.sum(insignificant[n],1),1)
        weights = torch.sum(weights,0)
            #t1[n]=torch.mul(t1[n],insignificant[n].type(torch.cuda.FloatTensor))
        #print(weights.size()) 128
        #dydw[m][n]-- b_s*higher*lower
        #update variables after having marked
        t2 = torch.zeros(10,BATCH_SIZE).cuda()
        for m in range(10):
            for n in range(N_layer):
                t2[m] += torch.sum(torch.sum(torch.abs(torch.div(dydw[m][n]*insignificant[n],torch.sqrt(t1[n]))),1),1)


        t2.transpose_(0,1)
        t3 = torch.sum(torch.mul(t2,t2),1)  # BATCH_SIZE

        t2.transpose_(0, 1) #

        # print(t2.size()) torch.Size([10, 128])
        # print(insignificant[0].size()) torch.Size([128, 1000, 784])

        #for calculation, we change dydw structure
        '''
        dydw_shape = []
        for n in range(len(dydw[0])):
            dydw_tensor = dydw[0][n]
            for m in range(1, 10):
                dydw_tensor = torch.cat((dydw_tensor,dydw[m][n]),1)
            dydw_shape.append(dydw_tensor)
        '''

        #print(insignificant[n].size())  torch.Size([128, upper, lower])
        #print(t1[0].size())
        # t2 [BATCH_SIZE*N_output]   [Batch_size*upper*lower]
        #dydw[m][n]           batch_size*upper*lower
        #print(t2[m].size())  torch.Size([128])

        b  = []
        t4 = []
        for k in range(10):
            temp = []
            for n in range(N_layer):
                t4 = dydw[0][n].clone().zero_().cuda()
                #temp.append(torch.div(dydw[m][n], t1[n]))
                for m in range(10):
                    t4.add_(t2[m].unsqueeze(1).unsqueeze(1)*insignificant[n]*torch.sign(dydw[m][n])*\
                    ((k==m)*t1[n] - dydw[k][n]*dydw[m][n])/torch.pow(t1[n],1.5))
                temp.append(torch.div(dydw[m][n], t1[n])+ weights.unsqueeze(1).unsqueeze(1)*t4/(t3.unsqueeze(1).unsqueeze(1)))
            b.append(temp)

        del t1
        del t2
        del t3

        #params = mlp.state_dict()
        #Forward pass

        params = mlp.state_dict()
        param_list = list(params.values())

        weights = []
        bias = []
        for i in range(N_layer):
            weights.append(param_list[2*i])
            bias.append(param_list[2*i+1])



        ry = []
        ry.append(torch.cuda.FloatTensor(N_out,BATCH_SIZE,N_in).zero_())
        ry.append(torch.cuda.FloatTensor(N_out,BATCH_SIZE,N_hidden).zero_())
        ry.append(torch.cuda.FloatTensor(N_out,BATCH_SIZE,1).zero_())
        rs = []


        for k in range(N_out):
            temp = []
            for u in range(1,N_layer):
                rs_tmp = torch.mm(ry[u - 1][k], torch.transpose(weights[u-1], 0, 1))+ \
                torch.squeeze(torch.bmm(b[k][u-1],y_out[2*u-2][0].data.unsqueeze(2)))
                temp.append(rs_tmp)
                print(rs_tmp.size())
                print(y_out[2*u][0].data.size())
                ry[u][k] = rs_tmp*((1-y_out[2*u][0].data)*y_out[2*u][0].data)  #need review
                #BATCH_SIZE*HIDDEN*
            print(ry[u][k].size())
            print(weights[u][k].unsqueeze(1).size())
            print(torch.index_select(b[k][u],1,torch.cuda.LongTensor([k])).size())

            print(torch.mm(ry[u][k],weights[u][k].unsqueeze(1)).size())
            print(torch.squeeze(torch.bmm(torch.index_select(b[k][u],1,torch.cuda.LongTensor([k])), y_out[2*u][0].data.unsqueeze(2))).size())
            rs_tmp = torch.mm(ry[u][k],weights[u][k].unsqueeze(1))\
                     +torch.squeeze(torch.bmm(torch.index_select(b[k][u],1,torch.cuda.LongTensor([k])), y_out[2*u][0].data.unsqueeze(2)),2) #should be caraful
            print(rs_tmp.size())
            temp.append(rs_tmp)
            rs.append(temp)
            ry[u+1][k] = rs_tmp * (1- torch.index_select(y_in[2*u+1].data,1 ,torch.cuda.LongTensor([k])))*torch.index_select(y_in[2*u+1].data,1 ,torch.cuda.LongTensor([k]))


        print(len(rs))


        rdks = []
        rdky_top = []
        rdky_low = []
        rdw = []


        for k in range(N_out):
            rdks_k = []
            temp = torch.index_select(y_in[3].data,1,torch.cuda.LongTensor([k]))

            #print((rs[k][-1]).size())
            #print(((1-2*temp)*temp*(1-temp)).size())
            rdks_tmp=(rs[k][-1]*(1-2*temp)*temp*(1-temp))  #[128*1] rs[2*k+1]
            print(weights[-1][k].size()) #500  checked
            print(torch.index_select(b[k][-1],1,torch.cuda.LongTensor([k])).size()) #128*1*500
            print(ks[k][0].size()) #128*10
            print(torch.unsqueeze(torch.index_select(ks[k][0],1,torch.cuda.LongTensor([k])),1).size())
            #rdks 128*1*500 * 500        +  128*1*500 * 128*1
            temp=rdks_tmp*weights[-1][k]+ \
                 torch.squeeze(torch.index_select(b[k][-1],1,torch.cuda.LongTensor([k])),1)*torch.index_select(ks[k][0],1,torch.cuda.LongTensor([k]))
            print((rdks_tmp*weights[-1][k]).size())
            print(torch.index_select(b[k][-1],1,torch.cuda.LongTensor([k])).size())
            print(torch.unsqueeze(torch.index_select(ks[k][0],1,torch.cuda.LongTensor([k])),1).size())
            rdky_top.append(temp) #rdky has ten 3d tensors with (128*1*500)
            #(batch_size * hidden) * (batch_size*1)  + (batch_size*hidden)*(batch_size*1)
            print(y_out[-2][0].data.size())
            print(ry[-2][k].size())
            print(torch.index_select(ks[k][0],1,torch.cuda.LongTensor([k])).size())
            temp = y_out[-2][0].data*rdks_tmp + ry[-2][k]*torch.index_select(ks[k][0],1,torch.cuda.LongTensor([k]))
            rdw.append(temp)

            #(batch_size * (1) * hidden)*batch_size*hidden +

            temp_y = y_in[1].data
            #print(((1 - 2 * temp_y) * temp_y * (1 - temp_y)).size())


            temp = rdky_top[k]*(1-y_out[-2][0].data)*y_out[-2][0].data +\
                   rs[k][-2]*(1-2*temp_y)*temp_y*(1-temp_y)*ks[k][1][0]
            #print((rdks[k] * weights[u][k]).size())


            rdks_k.append(temp)
            #rdks[k] is 128*500  W is
            #
            for u in range(N_layer-2,-1,-1):
                print(rdks_k[N_layer-2-u].size())
                print(weights[u].size())
                print((ks[k][N_layer-2-u+1].size())) #torch.Size([128, 500])
                print(b[k][u].size()) #torch.Size([128, 500, 784])
                #[batch*hidden]
                rdky_temp = torch.mm(rdks_k[N_layer-2-u], weights[u]) + \
                       torch.squeeze(torch.bmm(torch.unsqueeze(ks[k][N_layer-2-u+1],1),b[k][u]),1)
                     #orch.squeeze(torch.index_select(b[k][-1], 1, torch.cuda.LongTensor([k])), 1) * torch.index_select(ks[k][0], 1, torch.cuda.LongTensor([k]))

                #print(rdky_temp.size()) torch.Size([128, 784])
                print(rdks_k[N_layer - 2 - u].size())
                print(y_out[u][0].data.size())
                #print(torch.bmm(torch.unsqueeze(rdks_k[N_layer-2-u],2),torch.unsqueeze(y_out[u][0].data,1).size())
                rdw_temp =  torch.bmm(torch.unsqueeze(rdks_k[N_layer-2-u],2),torch.unsqueeze(y_out[u][0].data,1))\
                            + torch.bmm(torch.unsqueeze(ks[k][N_layer-2-u+1],2), torch.unsqueeze(ry[u][k],1))
                print(rdky_temp.size())

                y_temp = y_out[u][0].data
                print(ks[k][2*N_layer-2*u-1].size())
                if(u!=0):
                    rdks_k.append((1-y_temp)*y_temp*rdky_temp + rs[k][u-1]*(1-2*y_temp)*y_temp*(1-y_temp)*ks[k][2*N_layer-2*u+1][0])
            rdks.append(rdks_k)

            rdw = torch.cuda.FloatTensor()
            for k in range






            

        loss = loss_func(output, b_y)

        '''
        for i in range(BATCH_SIZE):
            output[i:i+1].backward(torch.ones(2,10),retain_variables=True)
            loss = loss_func(output,b_y)
            mlp.zero_grad()
        '''

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if nonideal_train:
            apply_format_inplace('FXP', linear[0].data, IL, FL)  # weight
            apply_format_inplace('FXP', linear[1].data, IL, FL)  # bias
            apply_format_inplace('FXP', linear2[0].data, IL, FL)  # weight
            apply_format_inplace('FXP', linear2[1].data, IL, FL)  # bias
            apply_format_inplace('FXP', out[0].data, IL, FL)  # weight
            apply_format_inplace('FXP', out[1].data, IL, FL)  # bias

        if en_ntv_train:
            apply_bitflip(linear[0].data, p_flip, IL, FL, flip_len)
            apply_bitflip(linear2[0].data, p_flip, IL, FL, flip_len)
            apply_bitflip(out[0].data, p_flip, IL, FL, flip_len)

        if step % 50 == 0:
            print('Epoch: ', epoch, '| train loss : %.4f' % loss.data[0])

        avg_loss = np.append(avg_loss, loss.data[0])

        '''
        if step % 120 == 0:
            print('printing!!!')
            train_loss_y = np.append(train_loss_y,loss.data[0])
            train_loss_x = np.append(train_loss_x,i)
            plt.plot(train_loss_x, train_loss_y, 'r-', lw=2)
            plt.pause(0.05)
            #plt.show(block=False)
            i = i + 1

        train_loss_x = train_loss_x.astype(int)
        '''

        '''
        if step % 50 == 0:
            test_output = mlp(test_x)
            pred_y = torch.max(test_output,1)[1].data.squeeze()
            accuracy = sum(pred_y == test_y)/float(test_y.size(0))
            print('Epoch: ',epoch,'| train loss : %.4f' %loss.data[0], '| test accuracy: %.2f' % accuracy)
        '''

    # per epoch updating......
    if en_plot:
        train_loss_y = np.append(train_loss_y, np.mean(avg_loss))
        train_loss_x = np.append(train_loss_x, epoch)
        plt.semilogy(train_loss_x, train_loss_y, 'r-', lw=2)
        plt.xlabel('Epoch')
        plt.ylabel('Training error')
        plt.title('IL = 4, FL = 14')
        plt.grid(True)
        plt.pause(0.05)

plt.show()

if save_model:
    print('Saving model......')
    torch.save(mlp, 'mlp_mnist_10epoch.pkl')
    # >>>>>>>>Test neural network performance

if load_model:
    print('Loading model......')
    mlp = torch.load('mlp_mnist_10epoch.pkl')

# >>>>>>>>>>>>>>>>>>>>>>>>Inference
linear = list(mlp.hidden1.parameters())
linear2 = list(mlp.hidden2.parameters())
out = list(mlp.out.parameters())

# >>>>>>>>Regulate trained neural network weight into fixed point counterpart, then do inference
if nonideal_inference:
    apply_format_inplace('FXP', linear[0].data, IL, FL)  # weight regulation
    apply_format_inplace('FXP', linear[1].data, IL, FL)  # bias regulation
    apply_format_inplace('FXP', linear2[0].data, IL, FL)  # weight regulation
    apply_format_inplace('FXP', linear2[1].data, IL, FL)  # bias regulation
    apply_format_inplace('FXP', out[0].data, IL, FL)  # weight regulation
    apply_format_inplace('FXP', out[1].data, IL, FL)  # bias regulation
    apply_format_inplace('FXP', test_x.data, IL, FL)  # input regulation
    # apply_bitflip(linear[0].data,0.5,3,10,13)

if en_ntv_inference:
    print("injecting bit flip fault model...")
    apply_bitflip(linear[0].data, p_flip, IL, FL, flip_len)
    apply_bitflip(linear2[0].data, p_flip, IL, FL, flip_len)
    apply_bitflip(out[0].data, p_flip, IL, FL, flip_len)
    # apply_bitflip(x,p,IL,FL,flip_length):

# print(type(test_x)) #<class 'torch.autograd.variable.Variable'>
# print(type(test_x.data)) #<class 'torch.FloatTensor'>


# >>>>>>>>add forward hooker on each layer if non-ideal inference
if nonideal_inference and (not nonideal_train):
    # print('applied!!!')
    mlp.hidden1.register_forward_hook(fixed_point_hook)
    mlp.activation.register_forward_hook(fixed_point_hook)
    mlp.hidden2.register_forward_hook(fixed_point_hook)
    mlp.activation2.register_forward_hook(fixed_point_hook)
    mlp.out.register_forward_hook(fixed_point_hook)

# >>>>>>>> Testing!!!
test_output = mlp(test_x)
pred_y = torch.max(test_output, 1)[1].data.squeeze()
accuracy = sum(pred_y == test_y) / float(test_y.size(0))

print(accuracy, 'prediction accuracy!')
print('End of testing!')
